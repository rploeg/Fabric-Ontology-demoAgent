"""
Binding Markdown Parser for extracting binding configurations.

Parses the binding instruction markdown files generated by the demo generator
to extract entity-table mappings, property configurations, and relationship
contextualizations.
"""

import re
import logging
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple
from enum import Enum


logger = logging.getLogger(__name__)


class BindingType(Enum):
    """Type of binding extracted from markdown."""
    STATIC = "static"
    TIMESERIES = "timeseries"


@dataclass
class ParsedPropertyMapping:
    """A property mapping extracted from markdown."""
    source_column: str
    target_property: str
    data_type: Optional[str] = None
    is_key: bool = False


@dataclass
class ParsedEntityBinding:
    """An entity binding configuration extracted from markdown."""
    entity_name: str
    entity_id: Optional[str] = None
    table_name: str = ""
    key_column: str = ""
    timestamp_column: Optional[str] = None  # For timeseries
    binding_type: BindingType = BindingType.STATIC
    property_mappings: List[ParsedPropertyMapping] = field(default_factory=list)

    def to_dict(self) -> Dict:
        """Convert to dictionary format."""
        return {
            "entity_name": self.entity_name,
            "entity_id": self.entity_id,
            "table_name": self.table_name,
            "key_column": self.key_column,
            "timestamp_column": self.timestamp_column,
            "binding_type": self.binding_type.value,
            "property_mappings": [
                {
                    "source_column": pm.source_column,
                    "target_property": pm.target_property,
                    "data_type": pm.data_type,
                    "is_key": pm.is_key,
                }
                for pm in self.property_mappings
            ],
        }


@dataclass
class ParsedRelationshipBinding:
    """
    A relationship binding (contextualization) configuration extracted from markdown.
    
    This represents a foreign key relationship between two entities through
    a bridge/fact table.
    """
    relationship_name: str
    relationship_id: Optional[str] = None
    source_entity: str = ""  # Entity type that the relationship starts from
    target_entity: str = ""  # Entity type that the relationship points to
    table_name: str = ""  # The table containing the foreign keys
    source_key_column: str = ""  # Column linking to source entity's key
    target_key_column: str = ""  # Column linking to target entity's key
    source_type: str = "lakehouse"  # "lakehouse" or "eventhouse"
    
    def to_dict(self) -> Dict:
        """Convert to dictionary format."""
        return {
            "relationship_name": self.relationship_name,
            "relationship_id": self.relationship_id,
            "source_entity": self.source_entity,
            "target_entity": self.target_entity,
            "table_name": self.table_name,
            "source_key_column": self.source_key_column,
            "target_key_column": self.target_key_column,
            "source_type": self.source_type,
        }


class BindingMarkdownParser:
    """
    Parser for extracting binding configurations from markdown files.

    Supports parsing binding instructions generated by fabric-ontology-demo-v2.yaml
    in formats like:
    - lakehouse-binding.md (static/non-timeseries bindings + relationships)
    - eventhouse-binding.md (timeseries bindings)
    """

    # Patterns for parsing markdown content - Entity bindings
    # Matches: "### 1.1 Product Entity" or "### 1.1 Product (15 rows)" or "### 3.1 ProductionBatch Timeseries Binding"
    ENTITY_HEADER_PATTERN = re.compile(
        r"^###?\s+\d+\.\d+\s+([A-Z][A-Za-z0-9_]+)(?:\s+Entity|\s*\(\d+\s*rows?\)|\s+Timeseries\s+Binding)",
        re.MULTILINE
    )
    
    # Alternative entity header pattern (simpler format)
    ENTITY_HEADER_ALT_PATTERN = re.compile(
        r"^##\s+(?:Entity:?\s+)?([A-Z][A-Za-z0-9_]+)",
        re.MULTILINE
    )

    # Table pattern - prioritize "Source Table" over just "Source"
    TABLE_PATTERN = re.compile(
        r"Source\s+Table\s*\|\s*`?([A-Za-z][A-Za-z0-9_]+)`?",
        re.IGNORECASE
    )
    
    # Fallback table pattern
    TABLE_FALLBACK_PATTERN = re.compile(
        r"(?:Table)\s*\|\s*`?([A-Za-z][A-Za-z0-9_]+)`?",
        re.IGNORECASE
    )
    
    # Alternative table pattern (prose format)
    TABLE_ALT_PATTERN = re.compile(
        r"(?:Source\s+Table|Table):\s*`?([A-Za-z][A-Za-z0-9_]+)`?",
        re.IGNORECASE
    )

    KEY_COLUMN_PATTERN = re.compile(
        r"(?:Key|Primary\s*Key|Entity\s*Key)(?:\s*Column)?\s*\|\s*`?([A-Za-z][A-Za-z0-9_]+)`?",
        re.IGNORECASE
    )
    
    KEY_COLUMN_ALT_PATTERN = re.compile(
        r"(?:Key|Primary\s*Key|Entity\s*Key)(?:\s*Column)?:\s*`?([A-Za-z][A-Za-z0-9_]+)`?",
        re.IGNORECASE
    )

    TIMESTAMP_PATTERN = re.compile(
        r"(?:Timestamp|Time\s*Column|Precise\s*Timestamp):\s*`?([A-Za-z][A-Za-z0-9_]+)`?",
        re.IGNORECASE
    )

    # Pattern for table rows in property mappings
    TABLE_ROW_PATTERN = re.compile(
        r"\|\s*`?([A-Za-z][A-Za-z0-9_]*)`?\s*\|\s*`?([A-Za-z][A-Za-z0-9_]*)`?\s*\|(?:\s*`?([A-Za-z0-9_]*)`?\s*\|)?",
    )

    # Patterns for relationship bindings
    # Matches: "### 5.1 produces (Facility → ProductionBatch)"
    RELATIONSHIP_HEADER_PATTERN = re.compile(
        r"^###?\s+\d+\.\d+\s+([a-zA-Z][a-zA-Z0-9_]*)\s*\(([A-Z][A-Za-z0-9_]+)\s*(?:→|->)+\s*([A-Z][A-Za-z0-9_]+)\)",
        re.MULTILINE
    )
    
    # Alternative pattern: "### Relationship: produces"
    RELATIONSHIP_HEADER_ALT_PATTERN = re.compile(
        r"^###?\s+(?:Relationship:?\s+)?([a-zA-Z][a-zA-Z0-9_]+)",
        re.MULTILINE
    )

    # Pattern to extract source/target from table rows
    SOURCE_TABLE_PATTERN = re.compile(
        r"Source\s+Table\s*\|\s*`?([A-Za-z][A-Za-z0-9_]+)`?",
        re.IGNORECASE
    )
    
    SOURCE_KEY_PATTERN = re.compile(
        r"Source\s+(?:Entity\s+)?Key\s+Column\s*\|\s*`?([A-Za-z][A-Za-z0-9_]+)`?",
        re.IGNORECASE
    )
    
    TARGET_KEY_PATTERN = re.compile(
        r"Target\s+(?:Entity\s+)?Key\s+Column\s*\|\s*`?([A-Za-z][A-Za-z0-9_]+)`?",
        re.IGNORECASE
    )

    def __init__(self, binding_type: BindingType = BindingType.STATIC):
        """
        Initialize the parser.

        Args:
            binding_type: Type of bindings to parse
        """
        self.binding_type = binding_type
        self._bindings: List[ParsedEntityBinding] = []
        self._relationship_bindings: List[ParsedRelationshipBinding] = []

    def parse_file(self, file_path: Path) -> List[ParsedEntityBinding]:
        """
        Parse a binding markdown file.

        Args:
            file_path: Path to the markdown file

        Returns:
            List of parsed entity bindings
        """
        if not file_path.exists():
            logger.warning(f"Binding file not found: {file_path}")
            return []

        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()

        return self.parse_content(content)

    def parse_content(self, content: str) -> List[ParsedEntityBinding]:
        """
        Parse binding configuration from markdown content.

        Args:
            content: Markdown content

        Returns:
            List of parsed entity bindings
        """
        self._bindings = []

        # Split content by entity sections
        sections = self._split_by_entities(content)

        for entity_name, section_content in sections.items():
            binding = self._parse_entity_section(entity_name, section_content)
            if binding:
                self._bindings.append(binding)

        logger.info(f"Parsed {len(self._bindings)} entity bindings")
        return self._bindings

    def _split_by_entities(self, content: str) -> Dict[str, str]:
        """Split content into sections by entity headers."""
        sections = {}

        # Find all entity headers
        matches = list(self.ENTITY_HEADER_PATTERN.finditer(content))

        for i, match in enumerate(matches):
            entity_name = match.group(1)
            start = match.start()

            # End is either the next entity header or end of content
            if i + 1 < len(matches):
                end = matches[i + 1].start()
            else:
                end = len(content)

            sections[entity_name] = content[start:end]

        return sections

    def _parse_entity_section(
        self,
        entity_name: str,
        content: str,
    ) -> Optional[ParsedEntityBinding]:
        """Parse a single entity section."""
        binding = ParsedEntityBinding(
            entity_name=entity_name,
            binding_type=self.binding_type,
        )

        # Extract table name - try "Source Table" first, then fallback patterns
        table_match = self.TABLE_PATTERN.search(content)
        if table_match:
            binding.table_name = table_match.group(1)
        else:
            # Try fallback pattern
            table_match = self.TABLE_FALLBACK_PATTERN.search(content)
            if table_match:
                binding.table_name = table_match.group(1)
            else:
                # Default to entity name as table name (common convention)
                binding.table_name = entity_name

        # Extract key column
        key_match = self.KEY_COLUMN_PATTERN.search(content)
        if key_match:
            binding.key_column = key_match.group(1)

        # Extract timestamp column (for timeseries)
        if self.binding_type == BindingType.TIMESERIES:
            ts_match = self.TIMESTAMP_PATTERN.search(content)
            if ts_match:
                binding.timestamp_column = ts_match.group(1)

        # Extract property mappings from tables - only look within "Property Mappings" section
        binding.property_mappings = self._parse_property_table(content, binding.key_column)

        # Only return if we have meaningful data
        if binding.table_name and (binding.property_mappings or binding.key_column):
            return binding

        return None

    def _parse_property_table(
        self,
        content: str,
        key_column: str = "",
    ) -> List[ParsedPropertyMapping]:
        """Parse property mappings from markdown table within Property Mappings section."""
        mappings = []
        seen_columns = set()
        
        # Common header row patterns to skip
        header_patterns = {
            "column", "source", "sourcecol", "sourcecolumn", "source_column",
            "setting", "property", "ontology", "ontologyproperty", "ontology_property",
            "type", "datatype", "data_type", "value"
        }
        
        # Patterns to skip - these are not property mapping columns
        skip_values = {
            "symptom", "cause", "solution", "description", "requirement",
            "issue", "fix", "error", "status", "count", "result"
        }
        
        # Try to find "Property Mappings" section and limit search to that
        prop_mapping_start = content.lower().find("property mappings")
        if prop_mapping_start != -1:
            # Find the end of this section (next heading or "Steps:" section)
            section_end = len(content)
            next_section_patterns = ["**steps:**", "### ", "## ", "---"]
            for pattern in next_section_patterns:
                idx = content.lower().find(pattern, prop_mapping_start + 20)
                if idx != -1 and idx < section_end:
                    section_end = idx
            search_content = content[prop_mapping_start:section_end]
        else:
            search_content = content

        for match in self.TABLE_ROW_PATTERN.finditer(search_content):
            # Table format is: | Ontology Property | Source Column | Type |
            # So group(1) is target_property, group(2) is source_column
            target_prop = match.group(1)
            source_col = match.group(2)
            data_type = match.group(3) if match.lastindex >= 3 else None

            # Skip header rows and duplicates
            if source_col.lower() in header_patterns:
                continue
            if target_prop.lower() in header_patterns:
                continue
            if source_col in seen_columns:
                continue

            seen_columns.add(source_col)

            mapping = ParsedPropertyMapping(
                source_column=source_col,
                target_property=target_prop,
                data_type=data_type,
                is_key=(source_col == key_column),
            )
            mappings.append(mapping)

        return mappings

    def get_bindings(self) -> List[ParsedEntityBinding]:
        """Get all parsed bindings."""
        return self._bindings.copy()

    def get_relationship_bindings(self) -> List[ParsedRelationshipBinding]:
        """Get all parsed relationship bindings."""
        return self._relationship_bindings.copy()

    def parse_relationships(self, content: str) -> List[ParsedRelationshipBinding]:
        """
        Parse relationship (contextualization) bindings from markdown content.
        
        Looks for sections like:
        ### 5.1 produces (Facility → ProductionBatch)
        | Setting | Value |
        | Relationship | produces |
        | Source Table | DimProductionBatch |
        | Source Entity Key Column | FacilityId |
        | Target Entity Key Column | BatchId |
        
        Args:
            content: Markdown content
            
        Returns:
            List of parsed relationship bindings
        """
        self._relationship_bindings = []
        
        # Find all relationship sections using the main pattern
        matches = list(self.RELATIONSHIP_HEADER_PATTERN.finditer(content))
        
        for i, match in enumerate(matches):
            rel_name = match.group(1)
            source_entity = match.group(2)
            target_entity = match.group(3)
            start = match.start()
            
            # Find the end of this section
            if i + 1 < len(matches):
                end = matches[i + 1].start()
            else:
                # Look for next major section (## header)
                next_section = re.search(r"^##\s+", content[start + 1:], re.MULTILINE)
                if next_section:
                    end = start + 1 + next_section.start()
                else:
                    end = len(content)
            
            section_content = content[start:end]
            
            # Parse the relationship section
            binding = self._parse_relationship_section(
                rel_name, source_entity, target_entity, section_content
            )
            if binding:
                self._relationship_bindings.append(binding)
        
        logger.info(f"Parsed {len(self._relationship_bindings)} relationship bindings")
        return self._relationship_bindings

    def _parse_relationship_section(
        self,
        rel_name: str,
        source_entity: str,
        target_entity: str,
        content: str,
    ) -> Optional[ParsedRelationshipBinding]:
        """Parse a single relationship section."""
        binding = ParsedRelationshipBinding(
            relationship_name=rel_name,
            source_entity=source_entity,
            target_entity=target_entity,
        )
        
        # Extract source table
        table_match = self.SOURCE_TABLE_PATTERN.search(content)
        if table_match:
            binding.table_name = table_match.group(1)
        
        # Extract source entity key column
        source_key_match = self.SOURCE_KEY_PATTERN.search(content)
        if source_key_match:
            binding.source_key_column = source_key_match.group(1)
        
        # Extract target entity key column
        target_key_match = self.TARGET_KEY_PATTERN.search(content)
        if target_key_match:
            binding.target_key_column = target_key_match.group(1)
        
        # Only return if we have meaningful data
        if binding.table_name and binding.source_key_column and binding.target_key_column:
            return binding
        
        logger.debug(f"Incomplete relationship binding for {rel_name}: table={binding.table_name}, source_key={binding.source_key_column}, target_key={binding.target_key_column}")
        return None

    def parse_file_with_relationships(
        self, file_path: Path
    ) -> Tuple[List[ParsedEntityBinding], List[ParsedRelationshipBinding]]:
        """
        Parse both entity bindings and relationship bindings from a file.
        
        Args:
            file_path: Path to the markdown file
            
        Returns:
            Tuple of (entity_bindings, relationship_bindings)
        """
        if not file_path.exists():
            logger.warning(f"Binding file not found: {file_path}")
            return [], []
        
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        
        entity_bindings = self.parse_content(content)
        relationship_bindings = self.parse_relationships(content)
        
        return entity_bindings, relationship_bindings


class RelationshipBindingParser:
    """
    Specialized parser for relationship contextualizations.
    
    This parser focuses specifically on extracting relationship bindings
    from markdown files, which define how entities are connected through
    fact/bridge tables.
    """
    
    def __init__(self, source_type: str = "lakehouse"):
        """
        Initialize the parser.
        
        Args:
            source_type: Default source type ("lakehouse" or "eventhouse")
        """
        self.source_type = source_type
        self._relationships: List[ParsedRelationshipBinding] = []
    
    def parse_file(self, file_path: Path) -> List[ParsedRelationshipBinding]:
        """
        Parse relationship bindings from a markdown file.
        
        Args:
            file_path: Path to the markdown file
            
        Returns:
            List of parsed relationship bindings
        """
        if not file_path.exists():
            logger.warning(f"Binding file not found: {file_path}")
            return []
        
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        
        return self.parse_content(content)
    
    def parse_content(self, content: str) -> List[ParsedRelationshipBinding]:
        """
        Parse relationship bindings from markdown content.
        
        Args:
            content: Markdown content
            
        Returns:
            List of parsed relationship bindings
        """
        parser = BindingMarkdownParser()
        return parser.parse_relationships(content)
    
    def get_relationships(self) -> List[ParsedRelationshipBinding]:
        """Get all parsed relationship bindings."""
        return self._relationships.copy()


def parse_demo_bindings(demo_path: Path) -> Dict[str, List]:
    """
    Parse all binding files from a demo package.

    Looks for:
    - bindings/lakehouse-binding.md → static bindings + relationship bindings
    - bindings/eventhouse-binding.md → timeseries bindings

    Args:
        demo_path: Path to the demo folder

    Returns:
        Dict with 'static', 'timeseries', and 'relationships' binding lists
    """
    # Case-insensitive folder discovery for bindings/ or Bindings/
    bindings_dir = None
    for variant in ["bindings", "Bindings"]:
        candidate = demo_path / variant
        if candidate.is_dir():
            bindings_dir = candidate
            break
    
    result = {
        "static": [],
        "timeseries": [],
        "relationships": [],
    }

    if not bindings_dir:
        logger.warning(f"Bindings directory not found in: {demo_path}")
        return result

    # Parse lakehouse (static) bindings and relationships
    lakehouse_binding = bindings_dir / "lakehouse-binding.md"
    if lakehouse_binding.exists():
        parser = BindingMarkdownParser(BindingType.STATIC)
        entity_bindings, relationship_bindings = parser.parse_file_with_relationships(
            lakehouse_binding
        )
        result["static"] = entity_bindings
        result["relationships"] = relationship_bindings

    # Parse eventhouse (timeseries) bindings
    eventhouse_binding = bindings_dir / "eventhouse-binding.md"
    if eventhouse_binding.exists():
        parser = BindingMarkdownParser(BindingType.TIMESERIES)
        result["timeseries"] = parser.parse_file(eventhouse_binding)

    return result


def parse_relationships_from_binding_file(file_path: Path) -> List[ParsedRelationshipBinding]:
    """
    Parse only relationship bindings from a binding markdown file.
    
    Args:
        file_path: Path to the markdown file
        
    Returns:
        List of parsed relationship bindings
    """
    parser = RelationshipBindingParser()
    return parser.parse_file(file_path)


def parse_binding_from_csv_headers(
    csv_path: Path,
    entity_name: Optional[str] = None,
    binding_type: BindingType = BindingType.STATIC,
) -> Optional[ParsedEntityBinding]:
    """
    Create a binding configuration by inferring from CSV headers.

    This is a fallback when binding markdown is not available.

    Args:
        csv_path: Path to CSV file
        entity_name: Entity name (defaults to filename stem)
        binding_type: Type of binding

    Returns:
        ParsedEntityBinding or None if parsing fails
    """
    import csv

    if not csv_path.exists():
        return None

    entity_name = entity_name or csv_path.stem

    try:
        with open(csv_path, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            headers = next(reader)

        if not headers:
            return None

        binding = ParsedEntityBinding(
            entity_name=entity_name,
            table_name=csv_path.stem,
            binding_type=binding_type,
        )

        # Try to identify key column (common patterns)
        key_patterns = ["id", "key", f"{entity_name.lower()}id", f"{entity_name.lower()}_id"]
        for header in headers:
            if header.lower() in key_patterns or header.lower().endswith("id"):
                binding.key_column = header
                break

        # Use first column as key if no key found
        if not binding.key_column and headers:
            binding.key_column = headers[0]

        # For timeseries, try to identify timestamp column
        if binding_type == BindingType.TIMESERIES:
            ts_patterns = ["timestamp", "time", "datetime", "precisetimestamp"]
            for header in headers:
                if header.lower() in ts_patterns or "time" in header.lower():
                    binding.timestamp_column = header
                    break

        # Create property mappings for all columns
        for header in headers:
            mapping = ParsedPropertyMapping(
                source_column=header,
                target_property=header,
                is_key=(header == binding.key_column),
            )
            binding.property_mappings.append(mapping)

        return binding

    except Exception as e:
        logger.warning(f"Failed to parse CSV headers from {csv_path}: {e}")
        return None
