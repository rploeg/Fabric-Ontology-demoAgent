"""
YAML Bindings Parser for structured binding configuration.

Parses the bindings.yaml file generated by the demo generator (v3.2+)
which provides machine-readable binding configurations for automation.

The YAML format is the SOURCE OF TRUTH for binding configuration,
while markdown files remain human-readable instructions only.
"""

import logging
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any

import yaml

from .binding_parser import (
    BindingType,
    ParsedEntityBinding,
    ParsedPropertyMapping,
    ParsedRelationshipBinding,
)


logger = logging.getLogger(__name__)


# Type mapping from bindings.yaml to KQL types
YAML_TO_KQL_TYPE_MAP = {
    "string": "string",
    "int": "int",
    "integer": "int",
    "double": "real",
    "float": "real",
    "boolean": "bool",
    "datetime": "datetime",
    "date": "datetime",
}


@dataclass
class EventhouseTableConfig:
    """Configuration for an Eventhouse/KQL table derived from bindings.yaml."""
    
    entity_name: str
    table_name: str
    key_column: str
    timestamp_column: str
    row_count: int
    columns: List[Dict[str, str]] = field(default_factory=list)  # [{"name": ..., "type": ...}]
    
    def to_kql_schema(self) -> str:
        """Generate KQL table schema for .create-merge command."""
        col_defs = ", ".join(
            f"{col['name']}: {col['type']}" for col in self.columns
        )
        return f".create-merge table {self.table_name} ({col_defs})"
    
    def get_csv_mapping(self) -> List[Dict[str, Any]]:
        """Generate CSV ingestion mapping based on CSV column order.

        The mapping maps CSV ordinal positions to KQL table columns,
        handling the fact that CSV files have Timestamp first while
        the KQL table schema has the key column first.
        """
        return [
            {"Name": col["name"], "DataType": col["type"], "Ordinal": col["csv_ordinal"]}
            for col in self.columns
            if col.get("csv_ordinal") is not None
        ]

    def to_csv_mapping_command(self, mapping_name: str | None = None) -> str:
        """Generate KQL command to create a CSV ingestion mapping.

        Returns a `.create-or-alter table ... ingestion csv mapping` command
        that maps CSV column positions to KQL columns.
        """
        import json
        name = mapping_name or f"{self.table_name}_csv"
        mapping = self.get_csv_mapping()
        return (
            f".create-or-alter table {self.table_name} "
            f"ingestion csv mapping '{name}' '{json.dumps(mapping)}'"
        )


@dataclass
class YamlBindingsConfig:
    """Parsed configuration from bindings.yaml."""
    
    version: str = "1.0"
    generator: str = ""
    
    # Lakehouse bindings
    lakehouse_entities: List[ParsedEntityBinding] = field(default_factory=list)
    lakehouse_relationships: List[ParsedRelationshipBinding] = field(default_factory=list)
    
    # Eventhouse bindings
    eventhouse_entities: List[ParsedEntityBinding] = field(default_factory=list)
    eventhouse_tables: List[EventhouseTableConfig] = field(default_factory=list)


class YamlBindingsParser:
    """
    Parser for bindings.yaml configuration files.
    
    This parser handles the structured YAML format introduced in v3.2 of the
    fabric-ontology-demo specification. It extracts both entity bindings and
    relationship bindings in a machine-readable format.
    """
    
    def __init__(self):
        self._config: Optional[YamlBindingsConfig] = None
    
    def parse_file(self, yaml_path: Path) -> YamlBindingsConfig:
        """
        Parse a bindings.yaml file.
        
        Args:
            yaml_path: Path to bindings.yaml
            
        Returns:
            Parsed bindings configuration
        """
        if not yaml_path.exists():
            logger.warning(f"Bindings YAML file not found: {yaml_path}")
            return YamlBindingsConfig()
        
        with open(yaml_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
        
        if not data:
            logger.warning(f"Empty bindings YAML file: {yaml_path}")
            return YamlBindingsConfig()
        
        return self.parse_dict(data)
    
    def parse_dict(self, data: Dict[str, Any]) -> YamlBindingsConfig:
        """
        Parse bindings from dictionary data.
        
        Args:
            data: Dictionary from YAML
            
        Returns:
            Parsed bindings configuration
        """
        # Support both 'version' and '_schema_version' for compatibility
        version = data.get("version") or data.get("_schema_version") or "1.0"
        config = YamlBindingsConfig(
            version=str(version),
            generator=data.get("generatedBy", ""),
        )
        
        # Parse lakehouse bindings
        lakehouse = data.get("lakehouse", {})
        if lakehouse:
            config.lakehouse_entities = self._parse_lakehouse_entities(
                lakehouse.get("entities", [])
            )
            config.lakehouse_relationships = self._parse_relationships(
                lakehouse.get("relationships", [])
            )
        
        # Parse eventhouse bindings
        eventhouse = data.get("eventhouse", {})
        if eventhouse:
            config.eventhouse_entities = self._parse_eventhouse_entities(
                eventhouse.get("entities", [])
            )
            config.eventhouse_tables = self._parse_eventhouse_tables(
                eventhouse.get("entities", [])
            )
        
        logger.info(
            f"Parsed bindings.yaml: "
            f"{len(config.lakehouse_entities)} lakehouse entities, "
            f"{len(config.lakehouse_relationships)} relationships, "
            f"{len(config.eventhouse_entities)} eventhouse entities"
        )
        
        self._config = config
        return config
    
    def _parse_lakehouse_entities(
        self, entities: List[Dict[str, Any]]
    ) -> List[ParsedEntityBinding]:
        """Parse lakehouse entity bindings."""
        result = []
        
        for entity_data in entities:
            binding = ParsedEntityBinding(
                entity_name=entity_data.get("entity", ""),
                table_name=entity_data.get("sourceTable", ""),
                key_column=entity_data.get("keyColumn", ""),
                binding_type=BindingType.STATIC,
            )
            
            # Parse property mappings
            for prop in entity_data.get("properties", []):
                mapping = ParsedPropertyMapping(
                    source_column=prop.get("column", ""),
                    target_property=prop.get("property", ""),
                    data_type=prop.get("type"),
                    is_key=(prop.get("column") == binding.key_column),
                )
                binding.property_mappings.append(mapping)
            
            result.append(binding)
        
        return result
    
    def _parse_eventhouse_entities(
        self, entities: List[Dict[str, Any]]
    ) -> List[ParsedEntityBinding]:
        """Parse eventhouse (timeseries) entity bindings."""
        result = []
        
        for entity_data in entities:
            binding = ParsedEntityBinding(
                entity_name=entity_data.get("entity", ""),
                table_name=entity_data.get("sourceTable", ""),
                key_column=entity_data.get("keyColumn", ""),
                timestamp_column=entity_data.get("timestampColumn"),
                binding_type=BindingType.TIMESERIES,
            )
            
            # Parse property mappings
            for prop in entity_data.get("properties", []):
                mapping = ParsedPropertyMapping(
                    source_column=prop.get("column", ""),
                    target_property=prop.get("property", ""),
                    data_type=prop.get("type"),
                    is_key=False,
                )
                binding.property_mappings.append(mapping)
            
            result.append(binding)
        
        return result
    
    def _parse_eventhouse_tables(
        self, entities: List[Dict[str, Any]]
    ) -> List[EventhouseTableConfig]:
        """
        Parse eventhouse entities into KQL table configurations.
        
        This extracts the schema information needed to create KQL tables
        and ingest data.
        """
        result = []
        
        for entity_data in entities:
            table_name = entity_data.get("sourceTable", "")
            key_column = entity_data.get("keyColumn", "")
            timestamp_column = entity_data.get("timestampColumn", "Timestamp")
            
            # ---- Build the canonical CSV column order ----
            # CSV files always have Timestamp first, then the key column,
            # followed by additional/property columns.  We read the CSV
            # header to get the true order when possible, but we also
            # build a fallback from the YAML spec.
            csv_columns_ordered: list[str] = []

            # Collect ALL declared columns from the YAML (properties +
            # additionalColumns) so we know every column that appears
            # in the CSV beyond Timestamp and key.
            property_cols = []
            for prop in entity_data.get("properties", []):
                col_name = prop.get("column", "")
                yaml_type = prop.get("type", "string")
                kql_type = YAML_TO_KQL_TYPE_MAP.get(yaml_type, "string")
                property_cols.append({"name": col_name, "type": kql_type})

            additional_cols = []
            for extra in entity_data.get("additionalColumns", []):
                col_name = extra.get("column", "")
                yaml_type = extra.get("type", "string")
                kql_type = YAML_TO_KQL_TYPE_MAP.get(yaml_type, "string")
                additional_cols.append({"name": col_name, "type": kql_type})

            # Try to read the actual CSV header for authoritative order
            csv_file_rel = entity_data.get("file", "")
            csv_header: list[str] | None = None
            if csv_file_rel:
                # The file path in YAML is relative to the demo folder;
                # we don't have demo_path here, so we record the relative
                # path and the caller (_parse_eventhouse_tables is called
                # from parse_dict which doesn't know demo_path).  We'll
                # fall back to YAML-declared order if reading fails.
                pass  # csv_header reading is handled below

            # Fallback CSV order: Timestamp, Key, ...additionalCols..., ...propertyCols...
            # This matches the generator convention.
            all_extra = additional_cols + property_cols
            # Remove duplicates of key/timestamp if they somehow appear
            all_extra = [
                c for c in all_extra
                if c["name"] not in (key_column, timestamp_column)
            ]

            # ---- Build KQL table columns (key first for KQL) ----
            columns = []
            columns.append({"name": key_column, "type": "string"})
            columns.append({"name": timestamp_column, "type": "datetime"})
            for c in all_extra:
                if c["name"] not in (key_column, timestamp_column):
                    columns.append(c)

            # ---- Assign csv_ordinal based on CSV column order ----
            # CSV order: Timestamp first, then key, then the rest in
            # the order they appear in the YAML (additional before properties
            # matches the generator output).
            csv_order = [timestamp_column, key_column] + [c["name"] for c in all_extra]
            csv_ordinal_map = {name: idx for idx, name in enumerate(csv_order)}

            for col in columns:
                col["csv_ordinal"] = csv_ordinal_map.get(col["name"])

            table_config = EventhouseTableConfig(
                entity_name=entity_data.get("entity", ""),
                table_name=table_name,
                key_column=key_column,
                timestamp_column=timestamp_column,
                row_count=entity_data.get("rowCount", 0),
                columns=columns,
            )
            
            result.append(table_config)
        
        return result
    
    def _parse_relationships(
        self, relationships: List[Dict[str, Any]]
    ) -> List[ParsedRelationshipBinding]:
        """Parse relationship bindings."""
        result = []
        
        for rel_data in relationships:
            binding = ParsedRelationshipBinding(
                relationship_name=rel_data.get("relationship", ""),
                source_entity=rel_data.get("sourceEntity", ""),
                target_entity=rel_data.get("targetEntity", ""),
                table_name=rel_data.get("sourceTable", ""),
                source_key_column=rel_data.get("sourceKeyColumn", ""),
                target_key_column=rel_data.get("targetKeyColumn", ""),
                source_type="lakehouse",
            )
            result.append(binding)
        
        return result
    
    def get_eventhouse_tables(self) -> List[EventhouseTableConfig]:
        """Get parsed eventhouse table configurations."""
        if self._config:
            return self._config.eventhouse_tables
        return []


def parse_bindings_yaml(demo_path: Path) -> Optional[YamlBindingsConfig]:
    """
    Parse bindings.yaml from a demo folder.
    
    Supports case-insensitive folder names (Bindings/ or bindings/).
    
    Args:
        demo_path: Path to the demo folder
        
    Returns:
        Parsed configuration or None if file doesn't exist
    """
    # Try case-insensitive folder discovery
    bindings_dir = None
    for variant in ["bindings", "Bindings"]:
        candidate = demo_path / variant
        if candidate.is_dir():
            bindings_dir = candidate
            break
    
    # Scan directory for any case variation
    if bindings_dir is None and demo_path.is_dir():
        for item in demo_path.iterdir():
            if item.is_dir() and item.name.lower() == "bindings":
                bindings_dir = item
                break
    
    if bindings_dir is None:
        logger.info(f"No bindings folder found in {demo_path}, will use markdown fallback")
        return None
    
    yaml_path = bindings_dir / "bindings.yaml"
    
    if not yaml_path.exists():
        logger.info(f"No bindings.yaml found at {yaml_path}, will use markdown fallback")
        return None
    
    parser = YamlBindingsParser()
    return parser.parse_file(yaml_path)


def get_eventhouse_table_configs(demo_path: Path) -> List[EventhouseTableConfig]:
    """
    Get Eventhouse table configurations from bindings.yaml.
    
    This is the primary entry point for the ingest_data step.
    
    Args:
        demo_path: Path to the demo folder
        
    Returns:
        List of EventhouseTableConfig for KQL table creation/ingestion
    """
    config = parse_bindings_yaml(demo_path)
    if config:
        return config.eventhouse_tables
    return []
